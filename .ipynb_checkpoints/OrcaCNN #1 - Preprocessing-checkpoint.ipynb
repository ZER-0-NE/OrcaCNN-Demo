{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Pre-Processing\n",
    "\n",
    "Real-world data is often noisy and requires pre-processing. To tackle this, we will apply some pre-processing steps to put all the samples in a standard format.\n",
    "\n",
    "- Resampling and normalization of all the audio samples: Since the sampling rates of all the audio samples will be different, we will resample all the audio files to a specific sampling frequency.\n",
    "\n",
    "- Data Augmentation techniques for padding of samples.\n",
    "\n",
    "- Removing all the dead samples (negligible frequency)(if any) from the dataset.\n",
    "\n",
    "- Denoising: We use Spectral-Subtraction method to reduce background noise. This method is based on spectral averaging and residual noise reduction, widely used for enhancement of noisy speech signals and can remove the stationary noise included in the sound. The Wiener filter is also an option.\n",
    "\n",
    "- We prepare spectrogram images with Linear Short Time Fourier Transform/Log-Mel Filter Bank features using Python. Librosa will be the preferred choice for computing these feature representations.\n",
    "\n",
    "- Contrast Enhancement: From past experience, Histogram Equalization has been found as a better option than capping the extreme values to mean ± 1.5 std.\n",
    "\n",
    "- Focusing on Low-Frequency Spectrum: The whale calls are normally found in the lower frequency spectrum ranging from 100Hz-200Hz. This would allow us to look only at the specific part of the image which will be beneficial to the CNN architecture when given as input. The rest of the image part would roughly count as “noise” (or irrelevant) portion for CNN.\n",
    "\n",
    "- Signal to Noise (SNR) Ratio: It is essential to have a high value of SNR for all the audio samples. Previous kaggle competition have shown this as an essential factor in improving the model accuracy.\n",
    "\n",
    "- Hydrophone data are subject to a variety of in-band noise sources. A band-pass filter is a simple way to remove unwanted noise outside of the signal frequency band. Wavelet denoising is an effective method for SNR improvement in environments with wide range of noise types competing for the same subspace.\n",
    "\n",
    "- To improve robustness to loudness variation, per-channel energy normalization (PCEN) was found better than the standard log-mel frontend (and even with an optimized spectral subtraction process). This provided a 24% reduction in error rate of whale call detection. It also helps in reducing the narrow-band noise which is most often caused by nearby boats and the equipment itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-94a6892ee766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from scipy.io import wavfile\n",
    "import wave\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import scipy\n",
    "from keras import losses, models, optimizers\n",
    "from keras.activations import relu, softmax\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n",
    "                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\n",
    "from keras.utils import Sequence, to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reading train and test class labels of the dataset from csv file.\n",
    "If we get a json file, we can easily convert it to csv and then proceed. \n",
    "'''\n",
    "train = pd.read_csv(\"whales_classlabel.csv\")\n",
    "# test = pd.read_csv(\"whales_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We reduce the number of negative examples here so as to balance out the whole dataset. The other method would be \n",
    "to augment the minority classes\n",
    "'''\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples=\", train.shape[0], \"  Number of classes=\", len(train.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('whales_classlabel.csv')\n",
    "df.set_index('fname', inplace=True)\n",
    "\n",
    "for f in df.index:\n",
    "    rate, signal = wavfile.read('data_labels/' + f)\n",
    "    #print(rate)\n",
    "    df.at[f, 'length'] = signal.shape[0]/rate\n",
    "classes = list(np.unique(df.label))\n",
    "class_dist = df.groupby(['label'])['length'].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Class Distribution', y=1.08)\n",
    "ax.pie(class_dist, labels = class_dist.index, autopct='%1.1f%%',\n",
    "    shadow=False, startangle=90)\n",
    "ax.axis('equal')\n",
    "plt.show()\n",
    "df.reset_index(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We observe that:\n",
    "\n",
    "The number of audio samples per category is non-uniform. The minimum number of audio samples in a category \n",
    "is 1 while the maximum is 6\n",
    "'''\n",
    "\n",
    "print('Minimum samples per category = ', min(train.label.value_counts()))\n",
    "print('Maximum samples per category = ', max(train.label.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We see that different samples have different durations of audio\n",
    "'''\n",
    "wav_list = glob.glob('data_labels/*.wav')\n",
    "#fname = 'data_labels/neg_00.wav'\n",
    "l = []\n",
    "for i in range(len(wav_list)):\n",
    "    wav = wave.open(wav_list[i])\n",
    "#     print(wav_list[i])\n",
    "    l.append([wav_list[i][12:], wav.getframerate(), wav.getnframes(), wav.getnframes()/wav.getframerate()])\n",
    "print(\"File  \", \"Sampling Rate  \", \"Total Samples  \", \"Duration(s) \" )\n",
    "for dat in l:\n",
    "    print(*dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data_labels/pos_14.wav'\n",
    "rate, data = wavfile.read(fname)\n",
    "print(\"Sampling (frame) rate = \", rate)\n",
    "print(\"Total samples (frames) = \", data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data, '-', );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at first 500 frames\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(data[:500], '.'); plt.plot(data[:500], '-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We see that the distribution of audio length across labels is non-uniform and has very high variance\n",
    "'''\n",
    "train['nframes'] = train['fname'].apply(lambda f: wave.open('data_labels/' + f).getnframes())\n",
    "# test['nframes'] = test['fname'].apply(lambda f: wave.open('data_test/' + f).getnframes())\n",
    "\n",
    "_, ax = plt.subplots(figsize=(16, 4))\n",
    "sns.violinplot(ax=ax, x=\"label\", y=\"nframes\", data=train, scale='width')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of audio frames, per label', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Majority of the audio files are short and there exist an outlier as seen below.\n",
    "'''\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\n",
    "train.nframes.hist(bins=100, ax=axes)\n",
    "plt.suptitle('Frame Length Distribution', ha='center', fontsize='large');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data_labels/pos_10.wav'\n",
    "data, sr = librosa.core.load(file_path, sr=44100,\n",
    "                                        res_type='kaiser_fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveplot(data, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(data)), ref=np.max)\n",
    "librosa.display.specshow(D, y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(D, y_axis='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(D, cmap='gray_r', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "audio_duration = 4\n",
    "use_mfcc = False\n",
    "n_mfcc = 20\n",
    "audio_length = sampling_rate * audio_duration\n",
    "preprocessing_fn = lambda x: x\n",
    "\n",
    "input_length = audio_length\n",
    "#print(input_length)\n",
    "\n",
    "# Random offset / Padding\n",
    "if len(data) > input_length:\n",
    "    max_offset = len(data) - input_length\n",
    "    offset = np.random.randint(max_offset)\n",
    "    data = data[offset:(input_length+offset)]\n",
    "else:\n",
    "    if input_length > len(data):\n",
    "        max_offset = input_length - len(data)\n",
    "        offset = np.random.randint(max_offset)\n",
    "    else:\n",
    "        offset = 0\n",
    "    data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "print(data.shape)\n",
    "# Normalization + Other Preprocessing\n",
    "if use_mfcc:\n",
    "    data = librosa.feature.mfcc(data, sr=sampling_rate,\n",
    "                                       n_mfcc=n_mfcc)\n",
    "    data = np.expand_dims(data, axis=-1)\n",
    "else:\n",
    "    data = preprocessing_fn(data)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveplot(data, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.cmap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "wav, _ = librosa.core.load(file_path, sr=SAMPLE_RATE)\n",
    "#wav = wav[:2*44100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(wav, sr = SAMPLE_RATE, n_mfcc=20)\n",
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 14))\n",
    "plt.imshow(mfcc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.abs(librosa.stft(wav, n_fft=2048, win_length=2000, hop_length=500))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(D, ref = np.max),y_axis='mel', x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_norm(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data-min_data)/(max_data-min_data+1e-6)\n",
    "    return data-0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data_labels/pos_14.wav'\n",
    "data, sr = librosa.core.load(file_path, sr=44100,\n",
    "                                        res_type='kaiser_fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data_labels/pos_15.wav'\n",
    "dataa, sr = librosa.core.load(file_path, sr=44100,\n",
    "                                        res_type='kaiser_fast')\n",
    "dataa_norm = audio_norm(dataa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = audio_norm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "audio_duration = 3\n",
    "use_mfcc = False\n",
    "n_mfcc = 20\n",
    "audio_length = sampling_rate * audio_duration\n",
    "preprocessing_fn = lambda x: x\n",
    "\n",
    "input_length = audio_length\n",
    "print(input_length)\n",
    "print(len(data))\n",
    "\n",
    "# Random offset / Padding\n",
    "if len(data) > input_length:\n",
    "    max_offset = len(data) - input_length\n",
    "    offset = np.random.randint(max_offset)\n",
    "    data = data[offset:(input_length+offset)]\n",
    "else:\n",
    "    if input_length > len(data):\n",
    "        max_offset = input_length - len(data)\n",
    "        offset = np.random.randint(max_offset)\n",
    "    else:\n",
    "        offset = 0\n",
    "    data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "\n",
    "# data = librosa.feature.mfcc(data, sr=sampling_rate,\n",
    "#                                                    n_mfcc=n_mfcc)\n",
    "# data = np.expand_dims(data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 14))\n",
    "plt.subplot(4,1,1)\n",
    "librosa.display.waveplot(data)\n",
    "plt.subplot(4,1,2)\n",
    "librosa.display.waveplot(data_norm)\n",
    "plt.subplot(4,1,3)\n",
    "librosa.display.waveplot(dataa)\n",
    "plt.subplot(4,1,4)\n",
    "librosa.display.waveplot(dataa_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 14))\n",
    "D1 = np.abs(librosa.stft(data, n_fft=1024, win_length=1024, hop_length=200))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(D1, ref = np.max),y_axis='mel', x_axis='time')\n",
    "D2 = np.abs(librosa.stft(data_norm, n_fft=1024, win_length=1024, hop_length=200))\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(D2, ref = np.max),y_axis='mel', x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
